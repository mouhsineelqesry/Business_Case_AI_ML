{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EvCcfwuSU-fz",
      "metadata": {
        "id": "EvCcfwuSU-fz"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6QR_RHvIVHT2",
      "metadata": {
        "id": "6QR_RHvIVHT2"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pl3dmH-EnJGl",
      "metadata": {
        "id": "pl3dmH-EnJGl"
      },
      "source": [
        "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vn6bbxSwVKl3",
      "metadata": {
        "id": "Vn6bbxSwVKl3"
      },
      "source": [
        "### Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jCIswL3zobj6",
      "metadata": {
        "id": "jCIswL3zobj6"
      },
      "source": [
        "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
        "\n",
        "As a member of the Data Science and AI team in the startup, you have been tasked with analyzing the data, developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZJOtDHVSF5hu",
      "metadata": {
        "id": "ZJOtDHVSF5hu"
      },
      "source": [
        "### Data Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZlkjI8V5F9RK",
      "metadata": {
        "id": "ZlkjI8V5F9RK"
      },
      "source": [
        "* `Date` : The date the news was released\n",
        "* `News` : The content of news articles that could potentially affect the company's stock price\n",
        "* `Open` : The stock price (in \\$) at the beginning of the day\n",
        "* `High` : The highest stock price (in \\$) reached during the day\n",
        "* `Low` :  The lowest stock price (in \\$) reached during the day\n",
        "* `Close` : The adjusted stock price (in \\$) at the end of the day\n",
        "* `Volume` : The number of shares traded during the day\n",
        "* `Label` : The sentiment polarity of the news content\n",
        "    * 1: positive\n",
        "    * 0: neutral\n",
        "    * -1: negative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VrFQHcW5mYgv",
      "metadata": {
        "id": "VrFQHcW5mYgv"
      },
      "source": [
        "## **Installing and Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-E2-iaumpo8",
      "metadata": {
        "id": "A-E2-iaumpo8"
      },
      "outputs": [],
      "source": [
        "# installing the sentence-transformers and gensim libraries for word embeddings\n",
        "!pip install -U sentence-transformers gensim transformers tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDKtLkEi2H7q",
      "metadata": {
        "id": "hDKtLkEi2H7q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TOLin0Zk0Qdx",
      "metadata": {
        "id": "TOLin0Zk0Qdx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wQ46zPgumfjF",
      "metadata": {
        "id": "wQ46zPgumfjF"
      },
      "source": [
        "## **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CrlkyDb9yNhn",
      "metadata": {
        "id": "CrlkyDb9yNhn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xwMwyupQ10gK",
      "metadata": {
        "id": "xwMwyupQ10gK"
      },
      "outputs": [],
      "source": [
        "path='/content/drive/MyDrive/courses_AI_ML/LLMs and prompt engineering /NLP_project/stock_news.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5tdwECvH2aA-",
      "metadata": {
        "id": "5tdwECvH2aA-"
      },
      "outputs": [],
      "source": [
        "reviws=pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vPdy5kBT2hvh",
      "metadata": {
        "id": "vPdy5kBT2hvh"
      },
      "outputs": [],
      "source": [
        "reviws.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64UtXu-l25MY",
      "metadata": {
        "id": "64UtXu-l25MY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "EvFNfrvGWthn",
      "metadata": {
        "id": "EvFNfrvGWthn"
      },
      "source": [
        "## **Data Overview**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbH_NB6oyQKT",
      "metadata": {
        "id": "bbH_NB6oyQKT"
      },
      "outputs": [],
      "source": [
        "df=reviws.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QNooJlgE3GVc",
      "metadata": {
        "id": "QNooJlgE3GVc"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsonZggG3J_2",
      "metadata": {
        "id": "TsonZggG3J_2"
      },
      "source": [
        "no missing value in the data !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MuMiw8N53ObN",
      "metadata": {
        "id": "MuMiw8N53ObN"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pw0ETPQ83ZkB",
      "metadata": {
        "id": "Pw0ETPQ83ZkB"
      },
      "source": [
        "no duplicate in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5jF6BMj04NGF",
      "metadata": {
        "id": "5jF6BMj04NGF"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CXMr9GqJ4QzA",
      "metadata": {
        "id": "CXMr9GqJ4QzA"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hMHpULBK4O48",
      "metadata": {
        "id": "hMHpULBK4O48"
      },
      "source": [
        "Date and News columns are object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WaoQsKbz4lR4",
      "metadata": {
        "id": "WaoQsKbz4lR4"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fy5BWZjc4oCP",
      "metadata": {
        "id": "Fy5BWZjc4oCP"
      },
      "source": [
        "the shape of the data is 349 rows and  8 columns , we say that data might be considered small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gcubOMF94nXP",
      "metadata": {
        "id": "gcubOMF94nXP"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hGHBK8-QeKOB",
      "metadata": {
        "id": "hGHBK8-QeKOB"
      },
      "source": [
        "## **Exploratory Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XRw6AJfm3xv9",
      "metadata": {
        "id": "XRw6AJfm3xv9"
      },
      "source": [
        "**Utile Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lU5wU4FY35hH",
      "metadata": {
        "id": "lU5wU4FY35hH"
      },
      "outputs": [],
      "source": [
        " # function to create labeled barplots\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hPB-aNkD37Rc",
      "metadata": {
        "id": "hPB-aNkD37Rc"
      },
      "outputs": [],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q0UlMQnyegl7",
      "metadata": {
        "id": "Q0UlMQnyegl7"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9GVt_AAbe29X",
      "metadata": {
        "id": "9GVt_AAbe29X"
      },
      "source": [
        "* Distribution of individual variables\n",
        "* Compute and check the distribution of the length of news content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HPreco9q7CVa",
      "metadata": {
        "id": "HPreco9q7CVa"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df, 'Open',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VeLbzp1r7TB9",
      "metadata": {
        "id": "VeLbzp1r7TB9"
      },
      "source": [
        "- The distribution of the stock opening prices is not normal.\n",
        "- the price of opening  stok's price of 60 is missing from the data\n",
        "- The box plot reveals four outliers, indicating high opening prices. These outliers are greater than 60.\n",
        "- Low opening stock prices are dominant. The majority of opening prices fall within the range of 20 to 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gm7irSWT9Lqz",
      "metadata": {
        "id": "Gm7irSWT9Lqz"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rF-DJ_779OTx",
      "metadata": {
        "id": "rF-DJ_779OTx"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df, 'High',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6jVHizrV9W_k",
      "metadata": {
        "id": "6jVHizrV9W_k"
      },
      "source": [
        "- The distributions are largely similar to the opening stock prices\n",
        "- We still lack data for stocks with a price of 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3suvkyc-i5V",
      "metadata": {
        "id": "d3suvkyc-i5V"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df, 'Low',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iMVAeTUq-n7S",
      "metadata": {
        "id": "iMVAeTUq-n7S"
      },
      "source": [
        "- Similar to the open and high features, the low prices exhibit the same patterns, with corresponding changes in count\n",
        "- The figure does not include stock prices close to 53. The stock price of 45 has a low count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VkkaRQM9_wHG",
      "metadata": {
        "id": "VkkaRQM9_wHG"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df, 'Close',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79GqsrM5_4js",
      "metadata": {
        "id": "79GqsrM5_4js"
      },
      "source": [
        "At closing, stock prices often revert to their opening levels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oa3MZuEsCM4I",
      "metadata": {
        "id": "oa3MZuEsCM4I"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df, 'Volume',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CyqbiqZrCP_s",
      "metadata": {
        "id": "CyqbiqZrCP_s"
      },
      "source": [
        "- The stock volume distribution is approximately normal with a right-skewed tail\n",
        "- One data point is considered an outlier and accounts for approximately 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Z5nGgH_HN2P",
      "metadata": {
        "id": "3Z5nGgH_HN2P"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(df,'Label',perc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vjC4hQ6PHRee",
      "metadata": {
        "id": "vjC4hQ6PHRee"
      },
      "source": [
        "The target variable is imbalanced. Class 0 comprises 48.7% of the data, class 1 comprises 28.4%, and class -1 comprises 22.9%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLE0s7OFKilB",
      "metadata": {
        "id": "hLE0s7OFKilB"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yn_9wfzxL-r1",
      "metadata": {
        "id": "Yn_9wfzxL-r1"
      },
      "source": [
        "* Correlation\n",
        "* Sentiment Polarity vs Price\n",
        "* Date vs Price\n",
        "\n",
        "**Note**: The above points are listed to provide guidance on how to approach bivariate analysis. Analysis has to be done beyond the above listed points to get maximum scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JjUkkaD9IXFl",
      "metadata": {
        "id": "JjUkkaD9IXFl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(df.select_dtypes(include=np.number).corr(),annot=True, vmin=-1, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bQVHaP6OI9qd",
      "metadata": {
        "id": "bQVHaP6OI9qd"
      },
      "source": [
        "- As observed in the invariant analysis, the open, high, low, and close features exhibit a strong correlation with a correlation coefficient of 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z4MgJVoIK_dS",
      "metadata": {
        "id": "z4MgJVoIK_dS"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df,hue='Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6mwuIvbCL8MM",
      "metadata": {
        "id": "6mwuIvbCL8MM"
      },
      "source": [
        "- The trading volume of stocks at opening and closing is generally similar  \n",
        "- No consistent relationship was found between the trading **volume** of stocks and **their price** fluctuations throughout the day  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CRBGeGrwLijK",
      "metadata": {
        "id": "CRBGeGrwLijK"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data=df,x='Label',y='Open')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DsUOE04wwEde",
      "metadata": {
        "id": "DsUOE04wwEde"
      },
      "source": [
        "- The negative label has an opening price median of around 43, with three outlier points.\n",
        "- The median opening price and minimum opening price for labels 0 and 1 are the same, but the maximum opening price for the stocks differs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N8z4-vOBmwqv",
      "metadata": {
        "id": "N8z4-vOBmwqv"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SeFOd8PKxlOT",
      "metadata": {
        "id": "SeFOd8PKxlOT"
      },
      "source": [
        "**FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xC2889iDxqhL",
      "metadata": {
        "id": "xC2889iDxqhL"
      },
      "source": [
        "We have three stock prices: **open**, **high**, **Low** and **close**. After univariate analysis and examining the correlation coefficient, we observe that these features are quite similar. We suggest taking **the mean** of these three features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oZwsFSxKyrJp",
      "metadata": {
        "id": "oZwsFSxKyrJp"
      },
      "outputs": [],
      "source": [
        "df1=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3nutDrxdzDee",
      "metadata": {
        "id": "3nutDrxdzDee"
      },
      "outputs": [],
      "source": [
        "df1['Mean_Price'] = df1[['Open', 'High','Low', 'Close']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XnKq4ualywdi",
      "metadata": {
        "id": "XnKq4ualywdi"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r4Nt7V9uy-sP",
      "metadata": {
        "id": "r4Nt7V9uy-sP"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(df1, 'Mean_Price',kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aXeozUQdzBzr",
      "metadata": {
        "id": "aXeozUQdzBzr"
      },
      "source": [
        "The distribution of the mean price of the stocks is quite similar to the distributions of the open, high, Low and close features.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jU8Qco9c0fuE",
      "metadata": {
        "id": "jU8Qco9c0fuE"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary features such as open, high, and close\n",
        "df1.drop(['Open', 'High','Low','Close'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kkryhH8T0yay",
      "metadata": {
        "id": "kkryhH8T0yay"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vkb-LHqczrrq",
      "metadata": {
        "id": "Vkb-LHqczrrq"
      },
      "source": [
        "For **feature engineering** of the '**Date**' feature, I will use **Cyclic Features**. This approach will help capture the periodic nature of days and months in a way that is meaningful for machine learning algorithms, especially when operating within the same year (2019)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NR-K_Yx81b7g",
      "metadata": {
        "id": "NR-K_Yx81b7g"
      },
      "outputs": [],
      "source": [
        "# Convert the date column to datetime\n",
        "df1['Date'] = pd.to_datetime(df1['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "syfMu0L41lm2",
      "metadata": {
        "id": "syfMu0L41lm2"
      },
      "outputs": [],
      "source": [
        "# Extract day and month\n",
        "df1['day'] = df1['Date'].dt.day\n",
        "df1['month'] = df1['Date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c5_PKnC12oA",
      "metadata": {
        "id": "0c5_PKnC12oA"
      },
      "outputs": [],
      "source": [
        "# Encode day as cyclic features\n",
        "df1['day_sin'] = np.sin(2 * np.pi * df1['day'] / 31)\n",
        "df1['day_cos'] = np.cos(2 * np.pi * df1['day'] / 31)\n",
        "\n",
        "# Encode month as cyclic features\n",
        "df1['month_sin'] = np.sin(2 * np.pi * df1['month'] / 12)\n",
        "df1['month_cos'] = np.cos(2 * np.pi * df1['month'] / 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z0PECRw027A4",
      "metadata": {
        "id": "Z0PECRw027A4"
      },
      "outputs": [],
      "source": [
        "df1.drop(['Date','day','month'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kC_0yyE12s7M",
      "metadata": {
        "id": "kC_0yyE12s7M"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YMGtpWy61mfY",
      "metadata": {
        "id": "YMGtpWy61mfY"
      },
      "outputs": [],
      "source": [
        "df2=df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_bnprM11hby",
      "metadata": {
        "id": "D_bnprM11hby"
      },
      "outputs": [],
      "source": [
        "X=df2.drop('Label',axis=1)\n",
        "y=df2['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S_kzm1wm1aqR",
      "metadata": {
        "id": "S_kzm1wm1aqR"
      },
      "outputs": [],
      "source": [
        "x_temp,X_test,y_temp,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06rW6AT11SH",
      "metadata": {
        "id": "e06rW6AT11SH"
      },
      "source": [
        "X_test and y_test will be reserved for evaluating the model. To avoid data leakage, I will keep them as they are and preprocess X_temp and y_temp data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kDHPuw622hI8",
      "metadata": {
        "id": "kDHPuw622hI8"
      },
      "outputs": [],
      "source": [
        "df2.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0rYgR14ORf7b",
      "metadata": {
        "id": "0rYgR14ORf7b"
      },
      "source": [
        "## **Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KKeXlVszNWT6",
      "metadata": {
        "id": "KKeXlVszNWT6"
      },
      "source": [
        "**APPROACH**:\n",
        "\n",
        "I will adopt two approaches for word embedding:\n",
        "\n",
        "- Use **Word2Vec** and **GloVe**.\n",
        "- Use encoder transformers from **Sentence Transformers**.\n",
        "\n",
        "\n",
        "At the end, I will evaluate each method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QscXlJChOPEr",
      "metadata": {
        "id": "QscXlJChOPEr"
      },
      "source": [
        "#### ***Word2Vec*** approch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JKpf8mDkOVZT",
      "metadata": {
        "id": "JKpf8mDkOVZT"
      },
      "source": [
        "**preprocessing** the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cp8RIZoMO5cK",
      "metadata": {
        "id": "Cp8RIZoMO5cK"
      },
      "outputs": [],
      "source": [
        "x_temp1=x_temp.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H1_lN_o0zKlW",
      "metadata": {
        "id": "H1_lN_o0zKlW"
      },
      "outputs": [],
      "source": [
        "#Removing special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kULx77s_PaKQ",
      "metadata": {
        "id": "kULx77s_PaKQ"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UI4DLkF1Os_B",
      "metadata": {
        "id": "UI4DLkF1Os_B"
      },
      "outputs": [],
      "source": [
        "# defining a function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    # Defining the regex pattern to match non-alphanumeric characters\n",
        "    pattern = '[^A-Za-z0-9]+'\n",
        "\n",
        "    # Finding the specified pattern and replacing non-alphanumeric characters with a blank string\n",
        "    new_text = ''.join(re.sub(pattern, ' ', text))\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-ctuoL-WOvx0",
      "metadata": {
        "id": "-ctuoL-WOvx0"
      },
      "outputs": [],
      "source": [
        "# Applying the function to remove special characters\n",
        "x_temp1['cleaned_news'] = x_temp1['News'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Icc2zVNPdGO",
      "metadata": {
        "id": "2Icc2zVNPdGO"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "x_temp1.loc[0:3, ['News','cleaned_news']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pd4GzXgTPxsD",
      "metadata": {
        "id": "Pd4GzXgTPxsD"
      },
      "outputs": [],
      "source": [
        "#Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VIBhSNFaPz_9",
      "metadata": {
        "id": "VIBhSNFaPz_9"
      },
      "outputs": [],
      "source": [
        "# changing the case of the text data to lower case\n",
        "x_temp1['cleaned_news'] = x_temp1['cleaned_news'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jd6K-zruQC2x",
      "metadata": {
        "id": "jd6K-zruQC2x"
      },
      "outputs": [],
      "source": [
        "#Removing extra whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Msdqlv_YQG6N",
      "metadata": {
        "id": "Msdqlv_YQG6N"
      },
      "outputs": [],
      "source": [
        "# removing extra whitespaces from the text\n",
        "x_temp1['cleaned_news'] = x_temp1['cleaned_news'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QTCqEmkjQVEE",
      "metadata": {
        "id": "QTCqEmkjQVEE"
      },
      "outputs": [],
      "source": [
        "#Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vFcaq86BQk0z",
      "metadata": {
        "id": "vFcaq86BQk0z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')    # loading the stopwords\n",
        "# nltk.download('punkt')    # loading the punkt module used in tokenization\n",
        "# nltk.download('omw-1.4')    # dependency for tokenization\n",
        "nltk.download('wordnet')    # loading the wordnet module that is used in stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aAqAEBh5QqYj",
      "metadata": {
        "id": "aAqAEBh5QqYj"
      },
      "outputs": [],
      "source": [
        "# to remove common stop words\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CH7hSb9IQYhh",
      "metadata": {
        "id": "CH7hSb9IQYhh"
      },
      "outputs": [],
      "source": [
        "# defining a function to remove stop words using the NLTK library\n",
        "def remove_stopwords(text):\n",
        "    # Split text into separate words\n",
        "    words = text.split()\n",
        "\n",
        "    # Removing English language stopwords\n",
        "    new_text = ' '.join([word for word in words if word not in stopwords.words('english')])\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EC2M1uq6Qb4N",
      "metadata": {
        "id": "EC2M1uq6Qb4N"
      },
      "outputs": [],
      "source": [
        "# Applying the function to remove stop words using the NLTK library\n",
        "x_temp1['cleaned_news_w_stpds'] = x_temp1['cleaned_news'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SyRsHuZ3UY_p",
      "metadata": {
        "id": "SyRsHuZ3UY_p"
      },
      "outputs": [],
      "source": [
        "# checking a couple of instances of cleaned data\n",
        "x_temp1.loc[0:3,['cleaned_news','cleaned_news_w_stpds']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hzBhR4kOUiO5",
      "metadata": {
        "id": "hzBhR4kOUiO5"
      },
      "outputs": [],
      "source": [
        "#Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Q5VsLntUskk",
      "metadata": {
        "id": "8Q5VsLntUskk"
      },
      "outputs": [],
      "source": [
        "# to perform stemming\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QlJPbN0XUl12",
      "metadata": {
        "id": "QlJPbN0XUl12"
      },
      "outputs": [],
      "source": [
        "# Loading the Porter Stemmer\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HnC0z8s_Uxi_",
      "metadata": {
        "id": "HnC0z8s_Uxi_"
      },
      "outputs": [],
      "source": [
        "# defining a function to perform stemming\n",
        "def apply_porter_stemmer(text):\n",
        "    # Split text into separate words\n",
        "    words = text.split()\n",
        "\n",
        "    # Applying the Porter Stemmer on every word of a message and joining the stemmed words back into a single string\n",
        "    new_text = ' '.join([ps.stem(word) for word in words])\n",
        "\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3EAYABcUUykP",
      "metadata": {
        "id": "3EAYABcUUykP"
      },
      "outputs": [],
      "source": [
        "# Applying the function to perform stemming\n",
        "x_temp1['final_cleaned_news'] = x_temp1['cleaned_news_w_stpds'].apply(apply_porter_stemmer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4445ZXV9VBgh",
      "metadata": {
        "id": "4445ZXV9VBgh"
      },
      "outputs": [],
      "source": [
        "x_temp1.loc[0:3,['cleaned_news','final_cleaned_news']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beVADP1VKcs",
      "metadata": {
        "id": "8beVADP1VKcs"
      },
      "source": [
        "###### **Text Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iYNJLRuqVPtA",
      "metadata": {
        "id": "iYNJLRuqVPtA"
      },
      "source": [
        "#### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y6E_LaG2bHN3",
      "metadata": {
        "id": "Y6E_LaG2bHN3"
      },
      "outputs": [],
      "source": [
        "# installing libraries to remove accented characters and use word embeddings\n",
        "!pip install unidecode gensim -q\n",
        "#!pip install --user unidecode gensim -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vH4_-wXFbC2y",
      "metadata": {
        "id": "vH4_-wXFbC2y"
      },
      "outputs": [],
      "source": [
        "# To import Word2Vec\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pKitP8gPVNOn",
      "metadata": {
        "id": "pKitP8gPVNOn"
      },
      "outputs": [],
      "source": [
        "# Creating a list of all words in our data\n",
        "words_list = [item.split(\" \") for item in x_temp1['final_cleaned_news'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weizf9UWVkcL",
      "metadata": {
        "id": "weizf9UWVkcL"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of Word2Vec\n",
        "vec_size = 300\n",
        "model_W2V = Word2Vec(words_list, vector_size = vec_size, min_count = 1, window=5, workers = 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gZ5mWFoxbVD5",
      "metadata": {
        "id": "gZ5mWFoxbVD5"
      },
      "outputs": [],
      "source": [
        "# Checking the size of the vocabulary\n",
        "print(\"Length of the vocabulary is\", len(list(model_W2V.wv.key_to_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKblU_3ccdaB",
      "metadata": {
        "id": "bKblU_3ccdaB"
      },
      "outputs": [],
      "source": [
        "# Retrieving the words present in the Word2Vec model's vocabulary\n",
        "words = list(model_W2V.wv.key_to_index.keys())\n",
        "\n",
        "# Retrieving word vectors for all the words present in the model's vocabulary\n",
        "wvs = model_W2V.wv[words].tolist()\n",
        "\n",
        "# Creating a dictionary of words and their corresponding vectors\n",
        "word_vector_dict = dict(zip(words, wvs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D4MtLdSvcekz",
      "metadata": {
        "id": "D4MtLdSvcekz"
      },
      "outputs": [],
      "source": [
        "def average_vectorizer_Word2Vec(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in words]\n",
        "\n",
        "    # adding the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(word_vector_dict[word])\n",
        "\n",
        "    # Dividing by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VyfZtyvvciqZ",
      "metadata": {
        "id": "VyfZtyvvciqZ"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe of the vectorized documents\n",
        "df_Word2Vec = pd.DataFrame(x_temp1['final_cleaned_news'].apply(average_vectorizer_Word2Vec).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "df_Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SQA1y4_LeY4y",
      "metadata": {
        "id": "SQA1y4_LeY4y"
      },
      "outputs": [],
      "source": [
        "df_Word2Vec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HdO8qoGJea01",
      "metadata": {
        "id": "HdO8qoGJea01"
      },
      "outputs": [],
      "source": [
        "x_temp1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9TUGBQUWdAUW",
      "metadata": {
        "id": "9TUGBQUWdAUW"
      },
      "outputs": [],
      "source": [
        "df_Word2Vec.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xEdS37_We9l6",
      "metadata": {
        "id": "xEdS37_We9l6"
      },
      "outputs": [],
      "source": [
        "x_temp1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9-El20RxJnU",
      "metadata": {
        "id": "e9-El20RxJnU"
      },
      "outputs": [],
      "source": [
        "# Ensure indexes are aligned\n",
        "df_Word2Vec.index = x_temp1.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mEa_BMVgfIsD",
      "metadata": {
        "id": "mEa_BMVgfIsD"
      },
      "outputs": [],
      "source": [
        "x_final = pd.concat([x_temp1, df_Word2Vec], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3KfpygqjfTzG",
      "metadata": {
        "id": "3KfpygqjfTzG"
      },
      "outputs": [],
      "source": [
        "x_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o9SPBrnjxSP7",
      "metadata": {
        "id": "o9SPBrnjxSP7"
      },
      "outputs": [],
      "source": [
        "x_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dTNTBj9MxWlk",
      "metadata": {
        "id": "dTNTBj9MxWlk"
      },
      "outputs": [],
      "source": [
        "x_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Q8nbz7PfXic",
      "metadata": {
        "id": "0Q8nbz7PfXic"
      },
      "outputs": [],
      "source": [
        "x_final1=x_final.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NTkW49Qdxoam",
      "metadata": {
        "id": "NTkW49Qdxoam"
      },
      "outputs": [],
      "source": [
        "x_final1.drop(['News','cleaned_news','cleaned_news_w_stpds','final_cleaned_news'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ePqKy0Ixtec",
      "metadata": {
        "id": "5ePqKy0Ixtec"
      },
      "outputs": [],
      "source": [
        "x_final1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6qJRxVlCx9bE",
      "metadata": {
        "id": "6qJRxVlCx9bE"
      },
      "source": [
        "#### GLOVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VsR9a138yI6w",
      "metadata": {
        "id": "VsR9a138yI6w"
      },
      "outputs": [],
      "source": [
        "# Converting the Stanford GloVe model vector format to word2vec\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = '/content/drive/MyDrive/courses_AI_ML/LLMs and prompt engineering /NLP_project/glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DCNebeHAl65",
      "metadata": {
        "id": "0DCNebeHAl65"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mj9GbqGqAnye",
      "metadata": {
        "id": "Mj9GbqGqAnye"
      },
      "outputs": [],
      "source": [
        "# Checking the size of the vocabulary\n",
        "print(\"Length of the vocabulary is\", len(glove_model.index_to_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WmG4lwTxBH-R",
      "metadata": {
        "id": "WmG4lwTxBH-R"
      },
      "outputs": [],
      "source": [
        "vec_size=100 # The GloVe model we used (glove.6B.100d.txt) provides word vectors with 100 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56xzK36hB2IJ",
      "metadata": {
        "id": "56xzK36hB2IJ"
      },
      "outputs": [],
      "source": [
        "glove_words = glove_model.index_to_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffpOHWD3B3dH",
      "metadata": {
        "id": "ffpOHWD3B3dH"
      },
      "outputs": [],
      "source": [
        "glove_word_vector_dict = dict(zip(glove_model.index_to_key,list(glove_model.vectors)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUBxtt9XBJ8o",
      "metadata": {
        "id": "MUBxtt9XBJ8o"
      },
      "outputs": [],
      "source": [
        "def average_vectorizer_GloVe(doc):\n",
        "    # Initializing a feature vector for the sentence\n",
        "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
        "\n",
        "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
        "    words_in_vocab = [word for word in doc.split() if word in glove_words]\n",
        "\n",
        "    # adding the vector representations of the words\n",
        "    for word in words_in_vocab:\n",
        "        feature_vector += np.array(glove_word_vector_dict[word])\n",
        "\n",
        "    # Dividing by the number of words to get the average vector\n",
        "    if len(words_in_vocab) != 0:\n",
        "        feature_vector /= len(words_in_vocab)\n",
        "\n",
        "    return feature_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ip3dy7NDBfGh",
      "metadata": {
        "id": "Ip3dy7NDBfGh"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe of the vectorized documents\n",
        "df_Glove = pd.DataFrame(x_temp1['final_cleaned_news'].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
        "df_Glove.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "siPMc3HbCcR6",
      "metadata": {
        "id": "siPMc3HbCcR6"
      },
      "outputs": [],
      "source": [
        "df_Glove.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KKIQxMsjCgbl",
      "metadata": {
        "id": "KKIQxMsjCgbl"
      },
      "outputs": [],
      "source": [
        "# align the two datasets\n",
        "df_Glove.index = x_temp1.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kIMqlzlvClOu",
      "metadata": {
        "id": "kIMqlzlvClOu"
      },
      "outputs": [],
      "source": [
        "# concate the two datasets\n",
        "df_Glove_final = pd.concat([x_temp1, df_Glove], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JXl913UvCstV",
      "metadata": {
        "id": "JXl913UvCstV"
      },
      "outputs": [],
      "source": [
        "df_Glove_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bPcVYNU2Cu5e",
      "metadata": {
        "id": "bPcVYNU2Cu5e"
      },
      "outputs": [],
      "source": [
        "df_Glove_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-wZx-6SxCyJu",
      "metadata": {
        "id": "-wZx-6SxCyJu"
      },
      "outputs": [],
      "source": [
        "df_Glove_final.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gir825zQC4N2",
      "metadata": {
        "id": "Gir825zQC4N2"
      },
      "outputs": [],
      "source": [
        "df_G_final2=df_Glove_final.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pui9i9QlC8eO",
      "metadata": {
        "id": "Pui9i9QlC8eO"
      },
      "outputs": [],
      "source": [
        "# drop the unncessery columns\n",
        "df_G_final2.drop(['News','cleaned_news','cleaned_news_w_stpds','final_cleaned_news'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OW1JHJuGDBy9",
      "metadata": {
        "id": "OW1JHJuGDBy9"
      },
      "outputs": [],
      "source": [
        "df_G_final2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4lwYN5bYmHp",
      "metadata": {
        "id": "f4lwYN5bYmHp"
      },
      "source": [
        "## **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zzURbnwDHq3q",
      "metadata": {
        "id": "zzURbnwDHq3q"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hvn8PxDmz063",
      "metadata": {
        "id": "Hvn8PxDmz063"
      },
      "outputs": [],
      "source": [
        "# Creating a function to plot the confusion matrix\n",
        "def plot_confusion_matrix(actual, predicted):\n",
        "    cm = confusion_matrix(actual, predicted)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    label_list = ['negative', 'neutral', 'positive']\n",
        "    sns.heatmap(cm, annot=True, fmt='.0f', xticklabels=label_list, yticklabels=label_list)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P-lJmfqGINjY",
      "metadata": {
        "id": "P-lJmfqGINjY"
      },
      "source": [
        "#### using word2vec vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z7i6Ogc1Iz-o",
      "metadata": {
        "id": "Z7i6Ogc1Iz-o"
      },
      "outputs": [],
      "source": [
        "x_train,x_val,y_train,y_val=train_test_split(x_final1,y_temp,test_size=0.2,random_state=1, stratify=y_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hB73dyaeIH4W",
      "metadata": {
        "id": "hB73dyaeIH4W"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KqgGJQccHuPw",
      "metadata": {
        "id": "KqgGJQccHuPw"
      },
      "outputs": [],
      "source": [
        "# Building the model\n",
        "rf_word2vec = RandomForestClassifier(n_estimators = 100, max_depth = 7, random_state = 1)\n",
        "\n",
        "# Fitting on train data\n",
        "rf_word2vec.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xaNig8yO5Zv7",
      "metadata": {
        "id": "xaNig8yO5Zv7"
      },
      "outputs": [],
      "source": [
        "# Predicting on train data\n",
        "y_pred_train = rf_word2vec.predict(x_train)\n",
        "\n",
        "# Predicting on test data\n",
        "y_pred_val = rf_word2vec.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xCYh1h_P5iYI",
      "metadata": {
        "id": "xCYh1h_P5iYI"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_pred_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sReBgo075pI6",
      "metadata": {
        "id": "sReBgo075pI6"
      },
      "source": [
        "The model predicted all of our labels correctly for the training data. Let's look at the validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ywAVtQm257nP",
      "metadata": {
        "id": "ywAVtQm257nP"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_val, y_pred_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OHFFK8As62q2",
      "metadata": {
        "id": "OHFFK8As62q2"
      },
      "source": [
        "The model failed to classify any of the positive sentiments correctly, this is exacty means that my model is overfitting !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8iH4_7HY83KQ",
      "metadata": {
        "id": "8iH4_7HY83KQ"
      },
      "source": [
        "let's try fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M5j1KJ_f8y_F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5j1KJ_f8y_F",
        "outputId": "174b1676-950d-413a-a5bb-e5041b11f874"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Balance class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
        "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False],\n",
        "}\n",
        "# Initialize the Random Forest model with class weights\n",
        "rf = RandomForestClassifier(random_state=1, class_weight=class_weights_dict)\n",
        "\n",
        "# Initialize the Randomized Search\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100, cv=5, verbose=2, random_state=1, n_jobs=-1)\n",
        "\n",
        "# Fit the Randomized Search to the data\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2yStrzij_TLF",
      "metadata": {
        "id": "2yStrzij_TLF"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "y_pred_v = random_search.predict(x_val)\n",
        "plot_confusion_matrix(y_val, y_pred_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3sOtq6itDOCU",
      "metadata": {
        "id": "3sOtq6itDOCU"
      },
      "outputs": [],
      "source": [
        "y_val.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V08kHDLZDZvH",
      "metadata": {
        "id": "V08kHDLZDZvH"
      },
      "source": [
        "Even with hyperparameter tuning, the model still struggles to predict the positive class. This is likely due to the small dataset and the small positive class, which consists of only 13 classes. Let's try different ensemble models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RYj-zFuZETvo",
      "metadata": {
        "id": "RYj-zFuZETvo"
      },
      "source": [
        "But first, let's use the **SMOTE** **technique** to augment the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OENMvoREEZr5",
      "metadata": {
        "id": "OENMvoREEZr5"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(sampling_strategy='minority', random_state=1)\n",
        "\n",
        "# Fit and apply SMOTE to your training data\n",
        "X_res, y_res = smote.fit_resample(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SUHPdFU-EsXY",
      "metadata": {
        "id": "SUHPdFU-EsXY"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the resampled training data\n",
        "from collections import Counter\n",
        "print(\"Original training set distribution:\", Counter(y_train))\n",
        "print(\"Resampled training set distribution:\", Counter(y_res))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zK0xmiwzEyoE",
      "metadata": {
        "id": "zK0xmiwzEyoE"
      },
      "source": [
        "The augmentation was applied to the positive and neutral classes as shown.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mCwUu9O_E_AO",
      "metadata": {
        "id": "mCwUu9O_E_AO"
      },
      "outputs": [],
      "source": [
        "# Train your RandomForest model on the resampled data\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=1)\n",
        "rf.fit(X_res, y_res)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = rf.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P74YbdwoFKL4",
      "metadata": {
        "id": "P74YbdwoFKL4"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_val, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hW4IuVNDFPkF",
      "metadata": {
        "id": "hW4IuVNDFPkF"
      },
      "source": [
        "With data augmentation, the model improved, but still did not achieve the goal !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vtFCDxOzFe-P",
      "metadata": {
        "id": "vtFCDxOzFe-P"
      },
      "source": [
        "###### **Ensemble Technique models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0GxrLTHQFejc",
      "metadata": {
        "id": "0GxrLTHQFejc"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Initialize individual models\n",
        "gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=1)\n",
        "ada = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
        "xgb = XGBClassifier(n_estimators=100, max_depth=3, use_label_encoder=False, eval_metric='mlogloss', random_state=1)\n",
        "\n",
        "# Combine them using a voting classifier\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('gb', gb),\n",
        "    ('ada', ada),\n",
        "    ('xgb', xgb)\n",
        "], voting='soft')  # 'hard' voting for majority voting, 'soft' for weighted voting\n",
        "\n",
        "# Fit the ensemble model\n",
        "ensemble.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred1 = ensemble.predict(x_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J-LQ0hgPGjHm",
      "metadata": {
        "id": "J-LQ0hgPGjHm"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "plot_confusion_matrix(y_val, y_pred1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iwUQoffWG4yf",
      "metadata": {
        "id": "iwUQoffWG4yf"
      },
      "source": [
        "those model struggle still !"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tnBG6EiMHF78",
      "metadata": {
        "id": "tnBG6EiMHF78"
      },
      "source": [
        "#### Glove vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BcmFmbF7HKMf",
      "metadata": {
        "id": "BcmFmbF7HKMf"
      },
      "source": [
        "We used Word2Vec vectors, but the models encountered difficulties with the classification task, especially for the positive class. Let's try GLOVE vectors and see if these models can better capture the patterns in the text news we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ao4_dK3UGi2v",
      "metadata": {
        "id": "Ao4_dK3UGi2v"
      },
      "outputs": [],
      "source": [
        "df_G_final2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zo5DrHRVIWnH",
      "metadata": {
        "id": "zo5DrHRVIWnH"
      },
      "outputs": [],
      "source": [
        "x_traing,x_valg,y_traing,y_valg=train_test_split(df_G_final2,y_temp,test_size=0.2,random_state=1, stratify=y_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xummrAUBIgpo",
      "metadata": {
        "id": "xummrAUBIgpo"
      },
      "outputs": [],
      "source": [
        "# Initialize individual models\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=1, class_weight='balanced')\n",
        "gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=1)\n",
        "ada = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
        "xgb = XGBClassifier(n_estimators=100, max_depth=3, use_label_encoder=False, eval_metric='mlogloss', random_state=1)\n",
        "\n",
        "# Combine them using a voting classifier\n",
        "ensembleg = VotingClassifier(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('gb', gb),\n",
        "    ('ada', ada),\n",
        "    ('xgb', xgb)\n",
        "], voting='hard')  # 'hard' voting for majority voting, 'soft' for weighted voting\n",
        "\n",
        "# Fit the ensemble model\n",
        "ensembleg.fit(x_traing, y_traing)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_predg = ensembleg.predict(x_valg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1DW_AR4NJyez",
      "metadata": {
        "id": "1DW_AR4NJyez"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_valg, y_predg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uGsgpAxzKYAX",
      "metadata": {
        "id": "uGsgpAxzKYAX"
      },
      "outputs": [],
      "source": [
        "# Combine them using a voting classifier\n",
        "ensemblegs = VotingClassifier(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('gb', gb),\n",
        "    ('ada', ada),\n",
        "    ('xgb', xgb)\n",
        "], voting='soft')  # 'hard' voting for majority voting, 'soft' for weighted voting\n",
        "\n",
        "# Fit the ensemble model\n",
        "ensemblegs.fit(x_traing, y_traing)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_predg = ensemblegs.predict(x_valg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xxqxYurLKd7F",
      "metadata": {
        "id": "xxqxYurLKd7F"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_valg, y_predg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ssshSlYKiUS",
      "metadata": {
        "id": "2ssshSlYKiUS"
      },
      "source": [
        "**The voting classifier** using **hard** voting performs better than **soft** voting. However, the classification is still not satisfactory, with the same incorrect classifications occurring. Nonetheless, **GLOVE vectors** **are superior** to **Word2Vec** vectors in capturing **the patterns** in the text data we have."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64lfX92iKFIP",
      "metadata": {
        "id": "64lfX92iKFIP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y_NUyA_qLvI4",
      "metadata": {
        "id": "Y_NUyA_qLvI4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and validation sets\n",
        "X_train_scaled = scaler.fit_transform(x_traing)\n",
        "X_val_scaled = scaler.transform(x_valg)\n",
        "\n",
        "# Now, train your model on the scaled data\n",
        "ensemble.fit(X_train_scaled, y_traing)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred_scaled = ensemble.predict(X_val_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9L_xtkcQMxUA",
      "metadata": {
        "id": "9L_xtkcQMxUA"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_valg, y_pred_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crYeW-0RM2kX",
      "metadata": {
        "id": "crYeW-0RM2kX"
      },
      "source": [
        "Even with standardization, the confusion matrix shows that the model is still not performing as expected. We initially thought this might be the reason, but after standardization, **trying different techniques and ensembles**, we realized that the technique used to convert the text to vectors could be a limitation. At this point, I will **use** an **encoder transformer**, which is a state-of-the-art method for capturing more meaning in the text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Li_x6ZK7PW79",
      "metadata": {
        "id": "Li_x6ZK7PW79"
      },
      "source": [
        "#### LSTM model classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zgkyma_eR4sK",
      "metadata": {
        "id": "zgkyma_eR4sK"
      },
      "outputs": [],
      "source": [
        "df3=df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qFKeEHd7oTF6",
      "metadata": {
        "id": "qFKeEHd7oTF6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0mhfGf6Z2epE",
      "metadata": {
        "id": "0mhfGf6Z2epE"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad the news column\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "text_sequences = tokenizer.texts_to_sequences(df3['News'])\n",
        "max_length = 100\n",
        "text_padded = pad_sequences(text_sequences, maxlen=max_length)\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_features = df.select()[['volume_of_stocks', 'price']]\n",
        "numerical_scaled = scaler.fit_transform(numerical_features)\n",
        "\n",
        "# One-hot encode the labels\n",
        "labels = get_dummies(df3['Label']).values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OdMo-A0c2woJ",
      "metadata": {
        "id": "OdMo-A0c2woJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define model inputs\n",
        "text_input = Input(shape=(max_length,))\n",
        "num_input = Input(shape=(numerical_scaled.shape[1],))\n",
        "\n",
        "# Text processing with LSTM\n",
        "embedding = Embedding(input_dim=30522, output_dim=128, input_length=max_length)(text_input)\n",
        "lstm = LSTM(units=128, return_sequences=True)(embedding)\n",
        "lstm = LSTM(units=64)(lstm)\n",
        "lstm = Dropout(0.5)(lstm)\n",
        "\n",
        "# Combine text and numerical features\n",
        "combined = Concatenate()([lstm, num_input])\n",
        "combined = Dense(64, activation='relu')(combined)\n",
        "combined = Dropout(0.5)(combined)\n",
        "output = Dense(3, activation='softmax')(combined)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, num_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train_text, X_val_text, X_train_num, X_val_num, y_train, y_val = train_test_split(\n",
        "    text_padded, numerical_scaled, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [X_train_text, X_train_num], y_train,\n",
        "    epochs=10, batch_size=32,\n",
        "    validation_data=([X_val_text, X_val_num], y_val)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate([X_val_text, X_val_num], y_val)\n",
        "print(f'Validation Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhQn20H1SAC2",
      "metadata": {
        "id": "bhQn20H1SAC2"
      },
      "outputs": [],
      "source": [
        "df3.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_M5ZkxAPPXdX",
      "metadata": {
        "id": "_M5ZkxAPPXdX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "text_data = df3['News']\n",
        "num_data = df3.drop(columns=['News', 'Label'])\n",
        "y = df3['Label']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000)  # Adjust num_words based on your dataset size\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "text_sequences = tokenizer.texts_to_sequences(text_data)\n",
        "\n",
        "# Pad the sequences to ensure equal length\n",
        "max_length = 100  # Based on your data analysis\n",
        "text_padded = pad_sequences(text_sequences, maxlen=max_length)\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "num_scaled = scaler.fit_transform(num_data)\n",
        "\n",
        "# Combine text and numerical data\n",
        "X_combined = np.hstack((text_padded, num_scaled))\n",
        "\n",
        "# One-Hot Encode the target variable\n",
        "y_one_hot = to_categorical(y + 1)  # Shift labels to 0, 1, 2 for one-hot encoding\n",
        "\n",
        "# Ensure the target variable has the same number of samples as the input data\n",
        "assert len(X_combined) == len(y_one_hot), \"Mismatch between input data and target variable samples.\"\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_combined, y_one_hot, test_size=0.2, random_state=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QjffqcrPZkvQ",
      "metadata": {
        "id": "QjffqcrPZkvQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define inputs\n",
        "text_input = Input(shape=(max_length,))\n",
        "num_input = Input(shape=(num_scaled.shape[1],))\n",
        "\n",
        "# Text processing with LSTM\n",
        "x = Embedding(input_dim=5000, output_dim=128, input_length=max_length)(text_input)\n",
        "x = LSTM(units=128, return_sequences=True)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = LSTM(units=64)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Combine text and numerical features\n",
        "combined = Concatenate()([x, num_input])\n",
        "\n",
        "# Add dense layers\n",
        "combined = Dense(64, activation='relu')(combined)\n",
        "combined = Dropout(0.5)(combined)\n",
        "output = Dense(3, activation='softmax')(combined)  # Three output units for three classes\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, num_input], outputs=output)\n",
        "\n",
        "# Compile the model with categorical crossentropy and custom metrics\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TGzr_x51abkI",
      "metadata": {
        "id": "TGzr_x51abkI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_predict = np.argmax(self.model.predict([X_val[:, :max_length], X_val[:, max_length:]]), axis=1)\n",
        "        val_targ = np.argmax(y_val, axis=1)\n",
        "        _val_f1 = f1_score(val_targ, val_predict, average='weighted')\n",
        "        print(f' — val_f1: {_val_f1:.4f}')\n",
        "\n",
        "# Train the model with the F1-Score callback\n",
        "history = model.fit(\n",
        "    [X_train[:, :max_length], X_train[:, max_length:]],\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(\n",
        "        [X_val[:, :max_length], X_val[:, max_length:]],\n",
        "        y_val\n",
        "    ),\n",
        "    callbacks=[F1ScoreCallback()]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MyhIUuqkfoJj",
      "metadata": {
        "id": "MyhIUuqkfoJj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.f1_scores = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_predict = np.argmax(self.model.predict([X_val[:, :max_length], X_val[:, max_length:]]), axis=1)\n",
        "        val_targ = np.argmax(y_val, axis=1)\n",
        "        _val_f1 = f1_score(val_targ, val_predict, average='weighted')\n",
        "        self.f1_scores.append(_val_f1)\n",
        "        print(f' — val_f1: {_val_f1:.4f}')\n",
        "\n",
        "# Initialize the callback\n",
        "f1_callback = F1ScoreCallback()\n",
        "\n",
        "# Train the model with the F1-Score callback\n",
        "history = model.fit(\n",
        "    [X_train[:, :max_length], X_train[:, max_length:]],\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(\n",
        "        [X_val[:, :max_length], X_val[:, max_length:]],\n",
        "        y_val\n",
        "    ),\n",
        "    callbacks=[f1_callback]\n",
        ")\n",
        "\n",
        "# Plotting the F1 scores\n",
        "plt.plot(f1_callback.f1_scores)\n",
        "plt.title('F1 Score per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0qP5KTLo3OOC",
      "metadata": {
        "id": "0qP5KTLo3OOC"
      },
      "source": [
        "## **Weekly News Summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UX5laeu27ZEt",
      "metadata": {
        "id": "UX5laeu27ZEt"
      },
      "source": [
        "**Important Note**: It is recommended to run this section of the project independently from the previous sections in order to avoid runtime crashes due to RAM overload."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oWvf3R3An5K4",
      "metadata": {
        "id": "oWvf3R3An5K4"
      },
      "source": [
        "#### Installing and Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7CZm5cGb3Qnq",
      "metadata": {
        "id": "7CZm5cGb3Qnq"
      },
      "outputs": [],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is being used\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.45 --force-reinstall --no-cache-dir -q\n",
        "\n",
        "# Installation for CPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is not being used\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.45 --force-reinstall --no-cache-dir -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6PSXsGEumNFe",
      "metadata": {
        "id": "6PSXsGEumNFe"
      },
      "outputs": [],
      "source": [
        "# For downloading the models from HF Hub\n",
        "!pip install huggingface_hub==0.20.3 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KcRnKYZnCulX",
      "metadata": {
        "id": "KcRnKYZnCulX"
      },
      "outputs": [],
      "source": [
        "# Function to download the model from the Hugging Face model hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Importing the Llama class from the llama_cpp module\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Function to download and load the model\n",
        "from tqdm import tqdm # For progress bar related functionalities\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fz8QdlSwYT8w",
      "metadata": {
        "id": "fz8QdlSwYT8w"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8_168morYStS",
      "metadata": {
        "id": "8_168morYStS"
      },
      "outputs": [],
      "source": [
        "# setting the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q5ACqb0C7lxn",
      "metadata": {
        "id": "q5ACqb0C7lxn"
      },
      "source": [
        "#### Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khGMDUAfnxap",
      "metadata": {
        "id": "khGMDUAfnxap"
      },
      "source": [
        "#### Llma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i3mS-CYPz4ll",
      "metadata": {
        "id": "i3mS-CYPz4ll"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\" # the model is in gguf format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sw0ZVHRPnBKf",
      "metadata": {
        "id": "sw0ZVHRPnBKf"
      },
      "outputs": [],
      "source": [
        "# Using hf_hub_download to download a model from the Hugging Face model hub\n",
        "# The repo_id parameter specifies the model name or path in the Hugging Face repository\n",
        "# The filename parameter specifies the name of the file to download\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VKXgW6yHnZH-",
      "metadata": {
        "id": "VKXgW6yHnZH-"
      },
      "outputs": [],
      "source": [
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # CPU cores\n",
        "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    # n_gpu_layers=43,  # uncomment and change this value based on GPU VRAM pool.\n",
        "    n_ctx=4096,  # Context window\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N3GB1awMn0ps",
      "metadata": {
        "id": "N3GB1awMn0ps"
      },
      "source": [
        "#### Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kZJ1prtpnsfy",
      "metadata": {
        "id": "kZJ1prtpnsfy"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x0KH2Fy6n7RX",
      "metadata": {
        "id": "x0KH2Fy6n7RX"
      },
      "outputs": [],
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hG5Peglcosmj",
      "metadata": {
        "id": "hG5Peglcosmj"
      },
      "outputs": [],
      "source": [
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=4024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xdwvokUzEI6Y",
      "metadata": {
        "id": "xdwvokUzEI6Y"
      },
      "outputs": [],
      "source": [
        "reviws.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IcWzI6szEYlL",
      "metadata": {
        "id": "IcWzI6szEYlL"
      },
      "outputs": [],
      "source": [
        "data=reviws.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ueZl6x_0m6dJ",
      "metadata": {
        "id": "ueZl6x_0m6dJ"
      },
      "source": [
        "#### Aggregating the data weekly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x2_Wb4rTz_cI",
      "metadata": {
        "id": "x2_Wb4rTz_cI"
      },
      "outputs": [],
      "source": [
        "data[\"Date\"] = pd.to_datetime(data['Date'])  # Convert the 'Date' column to datetime format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AmaZ-8wG0GKS",
      "metadata": {
        "id": "AmaZ-8wG0GKS"
      },
      "outputs": [],
      "source": [
        "# Group the data by week using the 'Date' column.\n",
        "weekly_grouped = data.groupby(pd.Grouper(key='Date', freq='W'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8O9L9hZy0mto",
      "metadata": {
        "id": "8O9L9hZy0mto"
      },
      "outputs": [],
      "source": [
        "weekly_grouped = weekly_grouped.agg(\n",
        "    {\n",
        "        'News': lambda x: ' || '.join(x)  # Join the news values with ' || ' separator.\n",
        "    }\n",
        ").reset_index()\n",
        "\n",
        "print(weekly_grouped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WtjfOgZ41bW7",
      "metadata": {
        "id": "WtjfOgZ41bW7"
      },
      "outputs": [],
      "source": [
        "weekly_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VRMFYZkESTqf",
      "metadata": {
        "id": "VRMFYZkESTqf"
      },
      "outputs": [],
      "source": [
        "weekly_grouped.News[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R31ZwDpRbcwh",
      "metadata": {
        "id": "R31ZwDpRbcwh"
      },
      "outputs": [],
      "source": [
        "# creating a copy of the data\n",
        "data_1=weekly_grouped.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V-mESljUbcwY",
      "metadata": {
        "id": "V-mESljUbcwY"
      },
      "source": [
        "#### Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AuoEISFH59m0",
      "metadata": {
        "id": "AuoEISFH59m0"
      },
      "source": [
        "**Note**:\n",
        "\n",
        "- The model is expected to summarize the news from the week by identifying the top three positive and negative events that are most likely to impact the price of the stock.\n",
        "\n",
        "- As an output, the model is expected to return a JSON containing two keys, one for Positive Events and one for Negative Events."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ottdZvWzWWY9",
      "metadata": {
        "id": "ottdZvWzWWY9"
      },
      "source": [
        "For the project, we need to define the prompt to be fed to the LLM to help it understand the task to perform. The following should be the components of the prompt:\n",
        "\n",
        "1. **Role**: Specifies the role the LLM will be taking up to perform the specified task, along with any specific details regarding the role\n",
        "\n",
        "  - **Example**: `You are an expert data analyst specializing in news content analysis.`\n",
        "\n",
        "2. **Task**: Specifies the task to be performed and outlines what needs to be accomplished, clearly defining the objective\n",
        "\n",
        "  - **Example**: `Analyze the provided news headline and return the main topics contained within it.`\n",
        "\n",
        "3. **Instructions**: Provides detailed guidelines on how to perform the task, which includes steps, rules, and criteria to ensure the task is executed correctly\n",
        "\n",
        "  - **Example**:\n",
        "\n",
        "```\n",
        "Instructions:\n",
        "1. Read the news headline carefully.\n",
        "2. Identify the main subjects or entities mentioned in the headline.\n",
        "3. Determine the key events or actions described in the headline.\n",
        "4. Extract relevant keywords that represent the topics.\n",
        "5. List the topics in a concise manner.\n",
        "```\n",
        "\n",
        "4. **Output Format**: Specifies the format in which the final response should be structured, ensuring consistency and clarity in the generated output\n",
        "\n",
        "  - **Example**: `Return the output in JSON format with keys as the topic number and values as the actual topic.`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1C7HTeXVYn8i",
      "metadata": {
        "id": "1C7HTeXVYn8i"
      },
      "source": [
        "**Full Prompt Example**:\n",
        "\n",
        "```\n",
        "You are an expert data analyst specializing in news content analysis.\n",
        "\n",
        "Task: Analyze the provided news headline and return the main topics contained within it.\n",
        "\n",
        "Instructions:\n",
        "1. Read the news headline carefully.\n",
        "2. Identify the main subjects or entities mentioned in the headline.\n",
        "3. Determine the key events or actions described in the headline.\n",
        "4. Extract relevant keywords that represent the topics.\n",
        "5. List the topics in a concise manner.\n",
        "\n",
        "Return the output in JSON format with keys as the topic number and values as the actual topic.\n",
        "```\n",
        "\n",
        "**Sample Output**:\n",
        "\n",
        "`{\"1\": \"Politics\", \"2\": \"Economy\", \"3\": \"Health\" }`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "re3HZr3BcB5v",
      "metadata": {
        "id": "re3HZr3BcB5v"
      },
      "source": [
        "##### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3Zy4GD2DcF9z",
      "metadata": {
        "id": "3Zy4GD2DcF9z"
      },
      "outputs": [],
      "source": [
        "# defining a function to parse the JSON output from the model\n",
        "def extract_json_data(json_str):\n",
        "    import json\n",
        "    try:\n",
        "        # Find the indices of the opening and closing curly braces\n",
        "        json_start = json_str.find('{')\n",
        "        json_end = json_str.rfind('}')\n",
        "\n",
        "        if json_start != -1 and json_end != -1:\n",
        "            extracted_category = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
        "            data_dict = json.loads(extracted_category)\n",
        "            return data_dict\n",
        "        else:\n",
        "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
        "            return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XwtB-hG4h9GL",
      "metadata": {
        "id": "XwtB-hG4h9GL"
      },
      "source": [
        "##### Defining the response function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wNXpBE7sDrr-",
      "metadata": {
        "id": "wNXpBE7sDrr-"
      },
      "outputs": [],
      "source": [
        "#Defining the response function\n",
        "def response_mistral_1(prompt, news):\n",
        "    model_output = llm(\n",
        "      f\"\"\"\n",
        "      [INST]\n",
        "      {prompt}\n",
        "      News Articles: {news}\n",
        "      [/INST]\n",
        "      \"\"\",\n",
        "      max_tokens=1024, #Complete the code to set the maximum number of tokens the model should generate for this task.\n",
        "      temperature=0.1, #Complete the code to set the value for temperature.\n",
        "      top_p=0.95, #Complete the code to set the value for top_p\n",
        "      top_k=50, #Complete the code to set the value for top_k\n",
        "      echo=False,\n",
        "    )\n",
        "\n",
        "    final_output = model_output[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_i1sqecC2yKy",
      "metadata": {
        "id": "_i1sqecC2yKy"
      },
      "outputs": [],
      "source": [
        "#Defining the response funciton for Task 1.\n",
        "def response_1(prompt,review):\n",
        "    model_output = llm(\n",
        "      f\"\"\"\n",
        "      Q: {prompt}\n",
        "      Review: {review}\n",
        "      A:\n",
        "      \"\"\",\n",
        "      max_tokens=32,\n",
        "      stop=[\"Q:\", \"\\n\"],\n",
        "      temperature=0.01,\n",
        "      echo=False,\n",
        "    )\n",
        "\n",
        "    temp_output = model_output[\"choices\"][0][\"text\"]\n",
        "\n",
        "    return temp_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JRsgiVSXbcwi",
      "metadata": {
        "id": "JRsgiVSXbcwi"
      },
      "source": [
        "##### Checking the model output on a sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "osPo6gq_0vZa",
      "metadata": {
        "id": "osPo6gq_0vZa"
      },
      "source": [
        "**Note**: Use this section to test out the prompt with one instance before using it for the entire weekly data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2do0h0ak3IuT",
      "metadata": {
        "id": "2do0h0ak3IuT"
      },
      "outputs": [],
      "source": [
        "# Define the prompt\n",
        "prompt = \"\"\"\n",
        "    You are an AI analyzing news articles about stock prices. Classify the sentiment of the provided news article into the following categories:\n",
        "    - Positive\n",
        "    - Negative\n",
        "    - Neutral\n",
        "\"\"\"\n",
        "\n",
        "# Use the first row of the news column\n",
        "first_news_article = data_1['News'].iloc[0]\n",
        "\n",
        "# Get the response\n",
        "response = response_1(prompt, first_news_article)\n",
        "print(\"First news article sentiment:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "erNvfAZs0o7B",
      "metadata": {
        "id": "erNvfAZs0o7B"
      },
      "outputs": [],
      "source": [
        "instruction_1 = \"\"\"\n",
        "    You are an AI analyzing news articles about stock prices. Classify the sentiment of the provided news article into one of the following categories:\n",
        "    - Positive\n",
        "    - Negative\n",
        "    - Neutral\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t93mVbr6Rp0c",
      "metadata": {
        "id": "t93mVbr6Rp0c"
      },
      "outputs": [],
      "source": [
        "data_1['model_response1'] = data_1['News'].apply(lambda x: response_mistral_1(instruction_1, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McBpHbEdU1e2",
      "metadata": {
        "id": "McBpHbEdU1e2"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nEBvt5evDm_6",
      "metadata": {
        "id": "nEBvt5evDm_6"
      },
      "outputs": [],
      "source": [
        "print(data_1.loc[4, 'model_response'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HxI4tTdvQ7Sb",
      "metadata": {
        "id": "HxI4tTdvQ7Sb"
      },
      "outputs": [],
      "source": [
        "data_1['model_response'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Af8Pp9WKTrFm",
      "metadata": {
        "id": "Af8Pp9WKTrFm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to extract and group sentiments into separate columns\n",
        "def parse_and_group_sentiments(model_response):\n",
        "    sentiment_dict = {'Positive': 0, 'Negative': 0, 'Neutral': 0}\n",
        "\n",
        "    # Split the response into individual parts\n",
        "    parts = model_response.split('      ')\n",
        "    for part in parts:\n",
        "        # Detect sentiment and corresponding text\n",
        "        if 'Positive:' in part:\n",
        "            sentiment_dict['Positive'] += 1\n",
        "        elif 'Negative:' in part:\n",
        "            sentiment_dict['Negative'] += 1\n",
        "        elif 'Neutral:' in part:\n",
        "            sentiment_dict['Neutral'] += 1\n",
        "\n",
        "    return pd.Series([sentiment_dict['Positive'], sentiment_dict['Negative'], sentiment_dict['Neutral']], index=['Positive', 'Negative', 'Neutral'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35kjwW9bmBml",
      "metadata": {
        "id": "35kjwW9bmBml"
      },
      "outputs": [],
      "source": [
        "# Improved function to parse and group sentiments into separate columns\n",
        "def parse_and_group_sentiments(model_response):\n",
        "    sentiment_dict = {'Positive': 0, 'Negative': 0, 'Neutral': 0}\n",
        "\n",
        "    # Split the response into individual parts\n",
        "    parts = model_response.split('\\n')\n",
        "\n",
        "    for part in parts:\n",
        "        # Detect sentiment and corresponding text\n",
        "        part_lower = part.lower()\n",
        "        if 'positive' in part_lower:\n",
        "            sentiment_dict['Positive'] += 1\n",
        "        elif 'negative' in part_lower:\n",
        "            sentiment_dict['Negative'] += 1\n",
        "        elif 'neutral' in part_lower:\n",
        "            sentiment_dict['Neutral'] += 1\n",
        "\n",
        "    return pd.Series([sentiment_dict['Positive'], sentiment_dict['Negative'], sentiment_dict['Neutral']], index=['Positive', 'Negative', 'Neutral'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "glYzduUwzjuN",
      "metadata": {
        "id": "glYzduUwzjuN"
      },
      "outputs": [],
      "source": [
        "# Function to extract top three positive and negative events\n",
        "def extract_top_events(model_response):\n",
        "    positive_events = []\n",
        "    negative_events = []\n",
        "\n",
        "    # Split the response into individual sentences\n",
        "    sentences = model_response.split('\\n')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if 'positive' in sentence.lower():\n",
        "            positive_events.append(sentence)\n",
        "        elif 'negative' in sentence.lower():\n",
        "            negative_events.append(sentence)\n",
        "\n",
        "    # Sort and select top three events\n",
        "    top_positive = positive_events[:3]\n",
        "    top_negative = negative_events[:3]\n",
        "\n",
        "    return pd.Series([top_positive, top_negative], index=['Top_Positive', 'Top_Negative'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T3O7fxo7TymU",
      "metadata": {
        "id": "T3O7fxo7TymU"
      },
      "outputs": [],
      "source": [
        "# Apply the function to extract sentiments and create separate columns\n",
        "data_1[['Positive', 'Negative', 'Neutral']] = data_1['model_response'].apply(parse_and_group_sentiments)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7jKE0VQlKa",
      "metadata": {
        "id": "6b7jKE0VQlKa"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C9JY4RaBpHXX",
      "metadata": {
        "id": "C9JY4RaBpHXX"
      },
      "outputs": [],
      "source": [
        "# Determine the dominant sentiment\n",
        "def dominant_sentiment(row):\n",
        "    sentiments = row[['Positive', 'Negative', 'Neutral']]\n",
        "    max_sentiment = sentiments.idxmax()\n",
        "    return max_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Djm7PSZKpI5i",
      "metadata": {
        "id": "Djm7PSZKpI5i"
      },
      "outputs": [],
      "source": [
        "data_1['Dominant_Sentiment'] = data_1.apply(dominant_sentiment, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5Naf6M6pNuf",
      "metadata": {
        "id": "b5Naf6M6pNuf"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FW_DeCr6pb4h",
      "metadata": {
        "id": "FW_DeCr6pb4h"
      },
      "outputs": [],
      "source": [
        "data_1.value_counts('Dominant_Sentiment')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DzwF_OIHtMSY",
      "metadata": {
        "id": "DzwF_OIHtMSY"
      },
      "source": [
        "#### find the top three positive and negative events that are most impact the stocts price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UhYt_y2CtDwh",
      "metadata": {
        "id": "UhYt_y2CtDwh"
      },
      "outputs": [],
      "source": [
        "instruction_2 = \"\"\"\n",
        "    Summarize the news from the week by identifying the top three positive and negative events that are most likely to impact the price of the stock.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dWvDt22DtFtV",
      "metadata": {
        "id": "dWvDt22DtFtV"
      },
      "outputs": [],
      "source": [
        "data_1['model_response'] = data_1['News'].apply(lambda x: response_mistral_1(instruction_2, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mOZdXZh6zgqv",
      "metadata": {
        "id": "mOZdXZh6zgqv"
      },
      "outputs": [],
      "source": [
        "# Apply the function to extract top events and create separate columns\n",
        "data_1[['Top_Positive', 'Top_Negative']] = data_1['model_response'].apply(extract_top_events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MqMUGmf6zyfV",
      "metadata": {
        "id": "MqMUGmf6zyfV"
      },
      "outputs": [],
      "source": [
        "data_1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jw0iM3G1z6tF",
      "metadata": {
        "id": "jw0iM3G1z6tF"
      },
      "outputs": [],
      "source": [
        "# Display the DataFrame with top positive and negative events\n",
        "data_1[['Date', 'Top_Positive', 'Top_Negative']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C-MW3XpmQfqN",
      "metadata": {
        "id": "C-MW3XpmQfqN"
      },
      "source": [
        "##### Formatting the model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cs3Ww3J0py_",
      "metadata": {
        "id": "2cs3Ww3J0py_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Function to return JSON output\n",
        "def get_json_output(df):\n",
        "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')  # Convert Timestamps to string format\n",
        "    json_output = df[['Date', 'Top_Positive', 'Top_Negative']].to_dict(orient='records')\n",
        "    return json.dumps(json_output, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K38eby2l0jVD",
      "metadata": {
        "id": "K38eby2l0jVD"
      },
      "outputs": [],
      "source": [
        "# Get JSON output\n",
        "json_output = get_json_output(data_1)\n",
        "print(json_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HiOLoD7BO3L-",
      "metadata": {
        "id": "HiOLoD7BO3L-"
      },
      "source": [
        "## **Conclusions and Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5r0z8qUZ4MOE",
      "metadata": {
        "id": "5r0z8qUZ4MOE"
      },
      "source": [
        "-\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uAOPv9oQtWcC",
      "metadata": {
        "id": "uAOPv9oQtWcC"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "EvCcfwuSU-fz",
        "6QR_RHvIVHT2",
        "Vn6bbxSwVKl3",
        "ZJOtDHVSF5hu",
        "VrFQHcW5mYgv",
        "EvFNfrvGWthn",
        "hGHBK8-QeKOB",
        "Q0UlMQnyegl7",
        "N8z4-vOBmwqv",
        "0rYgR14ORf7b",
        "f4lwYN5bYmHp",
        "q5ACqb0C7lxn",
        "re3HZr3BcB5v",
        "HiOLoD7BO3L-"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
